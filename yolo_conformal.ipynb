{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8c60b01-98ff-4aac-b260-db6514d006e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "%matplotlib inline\n",
    "import IPython\n",
    "from IPython.display import Image\n",
    "import cv2 as cv\n",
    "from confvision.models import *\n",
    "from confvision.conformalizer import RiskConformalizer, Conformalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7bf86bc-4369-4573-9bef-3549bf511802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Using torch 2.0.0(NVIDIA GeForce RTX 4060 Laptop GPU)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Setup complete. Using torch {torch.__version__}({torch.cuda.get_device_properties(0).name if torch.cuda.is_available()else 'CPU'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa31ddd-01d1-468b-a4ab-84423c940015",
   "metadata": {},
   "source": [
    "# Train the YOLOV5\n",
    "###  Use training set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb3a46b8-4df4-4446-a86e-265dd7e2c6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python train.py  --img 640 --batch 16 --epochs 50 --data /project/conf_data.yaml --weights yolov5s.pt --freeze 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cf2e8d-23bb-423b-9729-789a989d4931",
   "metadata": {},
   "source": [
    "# Apply to Calibration and Test set\n",
    "\n",
    "#### Move Both prediction in the folder /Preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44ee8148-eb45-44dc-a763-bb7be14ad8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python detect.py --weights runs/train/exp/weights/best.pt --img-size 640 --conf 0.4 --source /project/conformal_data/calibrate/images --save-conf --save-txt --hide-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a692b843-794d-4a20-8953-17be55f0cf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python detect.py --weights runs/train/exp/weights/best.pt --img-size 640 --conf 0.4 --source /project/conformal_data/test/images  --save-conf  --save-txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b31b7be-d7d2-4309-8aa8-9e075860fb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method : \"additive\" or \"multiplicative\"\n",
    "method = \"additive\"\n",
    "# Objectness threshold, between 0 and 1, default=0.3\n",
    "objectness_threshold = 0.3\n",
    "# IoU threshold, between 0 and 1, default=0.3\n",
    "iou_threshold = 0.3\n",
    "# Alpha, desired error rate, default=0.1\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dea26609-9e7f-4408-8b91-873f93892466",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLOv5()\n",
    "\n",
    "#cal_preds, test_preds = model.load_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76d5576c-e44d-45d9-9c2f-91f4e2476c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "images_folder = 'preds/calibrate/images'\n",
    "labels_folder = 'preds/calibrate/labels'\n",
    "prediction_folder = 'preds/cal_preds'\n",
    "\n",
    "images = []\n",
    "true_boxes = []\n",
    "scores_list = []\n",
    "pred_boxes = []\n",
    "\n",
    "\n",
    "for image_file in os.listdir(images_folder):\n",
    "    # Check if the file is an image (jpg or png)\n",
    "    if image_file.endswith('.jpg') or image_file.endswith('.png'):\n",
    "        image_path = os.path.join(images_folder, image_file)\n",
    "        image = cv2.imread(image_path)\n",
    "        images.append(image)\n",
    "\n",
    "        # Create pred_box and scores\n",
    "        txt_file = os.path.join(prediction_folder, os.path.splitext(image_file)[0] + '.txt')\n",
    "        try:\n",
    "            with open(txt_file, 'r') as file:\n",
    "                bounding_boxes = []\n",
    "                scores = []\n",
    "                for line in file:\n",
    "                    cls_id, x_center, y_center, width, height, score = map(float, line.strip().split())\n",
    "                    bounding_boxes.append([cls_id, x_center, y_center, width, height])\n",
    "                    scores.append(score)\n",
    "                pred_boxes.append(bounding_boxes)\n",
    "                scores_list.append(scores)\n",
    "        except FileNotFoundError:\n",
    "            # Create empty lists if the prediction file doesn't exist\n",
    "            pred_boxes.append(np.inf)\n",
    "            scores_list.append(np.inf)\n",
    "\n",
    "        # Create true_box\n",
    "        txt_file1 = os.path.join(labels_folder, os.path.splitext(image_file)[0] + '.txt')\n",
    "        try:\n",
    "            with open(txt_file1, 'r') as file:\n",
    "                bounding_boxes = []\n",
    "                for line in file:\n",
    "                    cls_id, x_center, y_center, width, height = map(float, line.strip().split())\n",
    "                    bounding_boxes.append([cls_id, x_center, y_center, width, height])\n",
    "                true_boxes.append(bounding_boxes)\n",
    "        except FileNotFoundError:\n",
    "            # Create an empty list if the label file doesn't exist\n",
    "            true_boxes.append([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78614258-2f41-4246-a1a9-daee6fe3cc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "images_folder = 'preds/test/images'\n",
    "labels_folder = 'preds/test/labels'\n",
    "prediction_folder = 'preds/test_preds'\n",
    "\n",
    "images_test = []\n",
    "true_boxes_test = []\n",
    "scores_list_test = []\n",
    "pred_boxes_test = []\n",
    "\n",
    "\n",
    "for image_file in os.listdir(images_folder):\n",
    "    # Check if the file is an image (jpg or png)\n",
    "    if image_file.endswith('.jpg') or image_file.endswith('.png'):\n",
    "        image_path = os.path.join(images_folder, image_file)\n",
    "        image = cv2.imread(image_path)\n",
    "        images_test.append(image)\n",
    "\n",
    "        # Create pred_box and scores\n",
    "        txt_file = os.path.join(prediction_folder, os.path.splitext(image_file)[0] + '.txt')\n",
    "        try:\n",
    "            with open(txt_file, 'r') as file:\n",
    "                bounding_boxes = []\n",
    "                scores = []\n",
    "                for line in file:\n",
    "                    cls_id, x_center, y_center, width, height, score = map(float, line.strip().split())\n",
    "                    bounding_boxes.append([cls_id, x_center, y_center, width, height])\n",
    "                    scores.append(score)\n",
    "                pred_boxes_test.append(bounding_boxes)\n",
    "                scores_list_test.append(scores)\n",
    "        except FileNotFoundError:\n",
    "            # Create empty lists if the prediction file doesn't exist\n",
    "            pred_boxes_test.append(np.inf)\n",
    "            scores_list_test.append(np.inf)\n",
    "\n",
    "        # Create true_box\n",
    "        txt_file1 = os.path.join(labels_folder, os.path.splitext(image_file)[0] + '.txt')\n",
    "        try:\n",
    "            with open(txt_file1, 'r') as file:\n",
    "                bounding_boxes = []\n",
    "                for line in file:\n",
    "                    cls_id, x_center, y_center, width, height = map(float, line.strip().split())\n",
    "                    bounding_boxes.append([cls_id, x_center, y_center, width, height])\n",
    "                true_boxes_test.append(bounding_boxes)\n",
    "        except FileNotFoundError:\n",
    "            # Create an empty list if the label file doesn't exist\n",
    "            true_boxes_test.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "077fee76-cc4f-4f72-aa65-ea1d020d8841",
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_preds = Predictions(images, true_boxes, pred_boxes, scores_list)\n",
    "test_preds = Predictions(images_test, true_boxes_test, pred_boxes_test, scores_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a7e2058-2681-4fbf-9f0a-69cf4968aade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.479079, 0.499927, 0.616486, 0.642935, 0.792116],\n",
       " [0.602402],\n",
       " [0.625652],\n",
       " [0.570343],\n",
       " [0.461389, 0.630898, 0.711298, 0.802868, 0.881529],\n",
       " [0.423235, 0.456668, 0.460682, 0.560427],\n",
       " [0.453903],\n",
       " [0.755155],\n",
       " [0.689696, 0.745215],\n",
       " inf,\n",
       " [0.439137, 0.841157, 0.842925],\n",
       " [0.445095],\n",
       " [0.796763],\n",
       " [0.516161],\n",
       " [0.643422],\n",
       " [0.406748, 0.425717],\n",
       " [0.600537],\n",
       " [0.538514],\n",
       " [0.407642, 0.438213, 0.617486],\n",
       " inf,\n",
       " [0.75764],\n",
       " [0.727441],\n",
       " [0.403224],\n",
       " [0.405881, 0.673151, 0.696145, 0.833126, 0.911176],\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " [0.930161],\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " [0.807879, 0.900144],\n",
       " inf,\n",
       " [0.600074],\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " [0.799141, 0.874911],\n",
       " [0.599943, 0.773765],\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " [0.538963, 0.847054],\n",
       " [0.594596, 0.710065, 0.901148],\n",
       " [0.858863, 0.880567],\n",
       " [0.559585, 0.913655],\n",
       " [0.848957, 0.884526],\n",
       " [0.788554, 0.807769, 0.854122],\n",
       " [0.819239, 0.82218],\n",
       " [0.850713],\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " [0.49115, 0.711697],\n",
       " [0.692259, 0.819824],\n",
       " [0.410826, 0.78112],\n",
       " [0.659058],\n",
       " [0.53446, 0.742988],\n",
       " [0.613911, 0.75594, 0.769968, 0.842211, 0.881017],\n",
       " [0.74357, 0.818232],\n",
       " [0.730765, 0.875257, 0.88261, 0.905703],\n",
       " [0.501005, 0.814989, 0.824021],\n",
       " inf,\n",
       " [0.610844, 0.711782],\n",
       " [0.841019],\n",
       " [0.64261],\n",
       " [0.410406, 0.897664],\n",
       " [0.636898, 0.845019],\n",
       " [0.580246, 0.631873],\n",
       " [0.644193],\n",
       " [0.754497],\n",
       " [0.65265, 0.783044],\n",
       " [0.939234, 0.966359],\n",
       " [0.921263, 0.964442],\n",
       " [0.927974, 0.963925],\n",
       " [0.964346],\n",
       " [0.927783],\n",
       " [0.956662],\n",
       " [0.950561],\n",
       " [0.945446],\n",
       " [0.949986],\n",
       " [0.952181],\n",
       " [0.937172],\n",
       " [0.939464],\n",
       " [0.927042],\n",
       " [0.927618],\n",
       " [0.924813],\n",
       " [0.940355],\n",
       " [0.952812],\n",
       " [0.929892],\n",
       " [0.923344],\n",
       " [0.93381],\n",
       " [0.9296],\n",
       " [0.960187],\n",
       " [0.933153],\n",
       " [0.949275],\n",
       " [0.948202],\n",
       " [0.921804],\n",
       " [0.926953, 0.941187],\n",
       " [0.936522, 0.944219],\n",
       " [0.936733, 0.942011],\n",
       " [0.918354, 0.953706],\n",
       " [0.924292, 0.957464],\n",
       " [0.905761, 0.952583],\n",
       " [0.807661, 0.913925, 0.953388],\n",
       " [0.937311, 0.951759],\n",
       " [0.752759, 0.921407],\n",
       " [0.844152, 0.938627],\n",
       " [0.92506],\n",
       " [0.933091],\n",
       " [0.936551],\n",
       " [0.945444],\n",
       " [0.949558],\n",
       " [0.947558],\n",
       " [0.940433],\n",
       " [0.946367],\n",
       " [0.943747, 0.952011],\n",
       " [0.901846, 0.94735],\n",
       " [0.933187, 0.935354],\n",
       " [0.934229, 0.958845],\n",
       " [0.934847, 0.959294],\n",
       " [0.917609, 0.938743],\n",
       " [0.911863, 0.957199],\n",
       " [0.879262, 0.941601, 0.961485],\n",
       " [0.903776, 0.959701, 0.963043],\n",
       " [0.877528, 0.959243, 0.961464],\n",
       " [0.917191, 0.926441, 0.927423],\n",
       " [0.932217, 0.939164, 0.949585],\n",
       " [0.559788, 0.929749, 0.950225],\n",
       " [0.576548, 0.945245, 0.948523],\n",
       " [0.822039, 0.941146],\n",
       " [0.882194, 0.948638],\n",
       " [0.43086, 0.78056],\n",
       " [0.785257, 0.924291, 0.93817],\n",
       " [0.846764, 0.878029, 0.955323],\n",
       " [0.838004, 0.91282, 0.929286],\n",
       " [0.807431, 0.833204, 0.902236, 0.912657, 0.943314],\n",
       " [0.827202, 0.901519, 0.903946, 0.916357, 0.95216],\n",
       " [0.637079, 0.837373, 0.902182, 0.913416, 0.961912],\n",
       " [0.931236],\n",
       " [0.941747],\n",
       " [0.948749],\n",
       " [0.9415],\n",
       " [0.930557],\n",
       " [0.928503],\n",
       " [0.933082],\n",
       " [0.923933],\n",
       " [0.958192],\n",
       " [0.946096],\n",
       " [0.937291],\n",
       " [0.954807],\n",
       " [0.942322],\n",
       " [0.935723],\n",
       " [0.93043],\n",
       " [0.934321],\n",
       " [0.94291],\n",
       " [0.947999],\n",
       " [0.94939],\n",
       " [0.944972],\n",
       " [0.930549],\n",
       " [0.953367],\n",
       " [0.969295],\n",
       " [0.899388, 0.924302, 0.940284],\n",
       " [0.899173, 0.904683, 0.948398],\n",
       " [0.832848, 0.937273],\n",
       " [0.914924, 0.939497],\n",
       " [0.933465, 0.939268],\n",
       " [0.944375],\n",
       " [0.928202],\n",
       " [0.932257],\n",
       " [0.930461],\n",
       " [0.933563],\n",
       " [0.920028],\n",
       " [0.936097, 0.944661],\n",
       " [0.936022, 0.943876],\n",
       " [0.902975, 0.936592],\n",
       " [0.900153, 0.937709],\n",
       " [0.93751],\n",
       " [0.933014],\n",
       " [0.931125],\n",
       " [0.919794],\n",
       " [0.795714, 0.950256],\n",
       " [0.884756, 0.911451],\n",
       " [0.930609],\n",
       " [0.916811],\n",
       " [0.925302],\n",
       " [0.926179],\n",
       " [0.909818],\n",
       " [0.915659],\n",
       " [0.914633],\n",
       " [0.917056],\n",
       " [0.922068],\n",
       " [0.923242],\n",
       " [0.920685, 0.933268],\n",
       " [0.92836, 0.937637],\n",
       " [0.936539],\n",
       " [0.936949],\n",
       " [0.502523, 0.931441],\n",
       " [0.869618, 0.94985],\n",
       " [0.855505, 0.888009],\n",
       " [0.850213, 0.955724],\n",
       " [0.824964, 0.952771],\n",
       " [0.949274],\n",
       " [0.944169],\n",
       " [0.930567],\n",
       " [0.921832, 0.923708],\n",
       " [0.937656, 0.950145],\n",
       " [0.928696, 0.957146],\n",
       " [0.925274, 0.956454],\n",
       " [0.935844, 0.939454],\n",
       " [0.900389, 0.943217],\n",
       " [0.934433, 0.94043],\n",
       " [0.930045, 0.952669],\n",
       " [0.882229, 0.936789],\n",
       " [0.925754, 0.937274],\n",
       " [0.925532, 0.940411],\n",
       " [0.934637, 0.941557],\n",
       " [0.932567, 0.953354],\n",
       " [0.943642, 0.951849],\n",
       " [0.934448, 0.941858],\n",
       " [0.935217, 0.944102],\n",
       " [0.938067],\n",
       " [0.843322, 0.848727]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_preds.scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c569fbcc-92b5-4260-9b5a-67f10eb18728",
   "metadata": {},
   "source": [
    "# Box-Wise Conformal Prediction\n",
    "## Max Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65585ef8-344e-40df-9337-0e7a75a29400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrating with alpha=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 0.225962, 0.453125, 0.144231, 0.367788], [0.0, 0.475962, 0.3125, 0.149038, 0.451923], [1.0, 0.223558, 0.455529, 0.144231, 0.372596], [1.0, 0.409856, 0.564904, 0.439904, 0.745192], [0.0, 0.774038, 0.532452, 0.375, 0.915865]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m conf_box_add_1c \u001b[38;5;241m=\u001b[39m Conformalizer(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbox\u001b[39m\u001b[38;5;124m\"\u001b[39m, method\u001b[38;5;241m=\u001b[39mmethod, coordinate_wise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mconf_box_add_1c\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalibrate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcal_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjectness_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjectness_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                          \u001b[49m\u001b[43miou_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miou_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m conf_box_add_1c\u001b[38;5;241m.\u001b[39mconformalize(test_preds);\n\u001b[1;32m      5\u001b[0m cal_preds\u001b[38;5;241m.\u001b[39mdescribe(objectness_threshold\u001b[38;5;241m=\u001b[39mobjectness_threshold, iou_threshold\u001b[38;5;241m=\u001b[39miou_threshold)\n",
      "File \u001b[0;32m/project/conformal_railway_signal_detection/yolov5/confvision/conformalizer.py:176\u001b[0m, in \u001b[0;36mConformalizer.calibrate\u001b[0;34m(self, preds, alpha, objectness_threshold, iou_threshold)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmargin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReplacing previously computed lambda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 176\u001b[0m Qs, conf_boxes, residuals \u001b[38;5;241m=\u001b[39m \u001b[43mconformalize_preds\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrue_boxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpred_boxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjectness_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjectness_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mIOU_THRESHOLD\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miou_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoordinate_wise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoordinate_wise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresiduals \u001b[38;5;241m=\u001b[39m residuals\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmargin \u001b[38;5;241m=\u001b[39m Qs\n",
      "File \u001b[0;32m/project/conformal_railway_signal_detection/yolov5/confvision/conformalizer.py:89\u001b[0m, in \u001b[0;36mconformalize_preds\u001b[0;34m(images, true_boxes, pred_boxes, pred_objs, IOU_THRESHOLD, objectness_threshold, alpha, tqdm_on, replace_iou, method, coordinate_wise)\u001b[0m\n\u001b[1;32m     85\u001b[0m tp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tb \u001b[38;5;129;01min\u001b[39;00m tbs:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# convert format\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m     p1, p2 \u001b[38;5;241m=\u001b[39m \u001b[43mtb\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpoints\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     90\u001b[0m     tb \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([p1[\u001b[38;5;241m0\u001b[39m], p1[\u001b[38;5;241m1\u001b[39m], p2[\u001b[38;5;241m0\u001b[39m], p2[\u001b[38;5;241m1\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, pb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pbs):\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "conf_box_add_1c = Conformalizer(mode=\"box\", method=method, coordinate_wise=False)\n",
    "conf_box_add_1c.calibrate(cal_preds, objectness_threshold=objectness_threshold,\n",
    "                          iou_threshold=iou_threshold, alpha=alpha)\n",
    "conf_box_add_1c.conformalize(test_preds);\n",
    "cal_preds.describe(objectness_threshold=objectness_threshold, iou_threshold=iou_threshold)\n",
    "test_preds.describe(objectness_threshold=objectness_threshold, iou_threshold=iou_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35889c21-0324-44fe-80ff-2db426bab5b4",
   "metadata": {},
   "source": [
    "## Bonferroni Correction\n",
    "This approach uses a Bonferroni correct to compute a margin in each direction, possibly resulting in boxes that fit the ground truth better, but at a cost of being overly conservative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ff03c7-7eda-450e-a9fd-4fa57554380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_box_add_4c = Conformalizer(mode=\"box\", method=method, coordinate_wise=True)\n",
    "conf_box_add_4c.calibrate(cal_preds, objectness_threshold=objectness_threshold,\n",
    "                          iou_threshold=iou_threshold, alpha=alpha)\n",
    "conf_box_add_4c.conformalize(test_preds);\n",
    "cal_preds.describe(objectness_threshold=objectness_threshold, iou_threshold=iou_threshold);\n",
    "test_preds.describe(objectness_threshold=objectness_threshold, iou_threshold=iou_threshold);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a452914-f36f-4152-946e-d92fc7fec815",
   "metadata": {},
   "source": [
    "# Image-wise Conformal Prediction and Conformal Risk Control\n",
    "## Conformal Prediction with Hausdorff Distance\n",
    "Intuitively, this method first proposed by De Grancey et al., aims to guarantee that, in expectation, 90% of images have at least 75% of their ground truth boxes included in their conformalized boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743dd96e-78c7-4d57-8abf-859ac21c1b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
